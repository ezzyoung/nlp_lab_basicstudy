{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "import re\n",
        "import datasets\n",
        "import tqdm\n",
        "import math\n",
        "from functools import partial\n",
        "import math\n",
        "import argparse\n",
        "import os\n",
        "import collections\n",
        "import json\n",
        "import sentencepiece\n",
        "import shutil\n",
        "import copy\n",
        "import multiprocessing\n",
        "import transformers\n",
        "from dataclasses import dataclass, field\n",
        "from evaluate import load\n",
        "\n",
        "# set \"high\" if you have a GPU with compute capability >= 8.0 else \"highest\"\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.10.0+cpu c:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\__init__.py\n",
            "5.1.0\n",
            "0.4.6\n",
            "evaluate.load import OK\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__, torch.__file__)\n",
        "\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "import evaluate\n",
        "print(evaluate.__version__)\n",
        "\n",
        "from evaluate import load\n",
        "print(\"evaluate.load import OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "## you can modify some options such as batch_size, depending on your environments  \n",
        "# 텍스트 데이터 -> (batch 크기, 문장 길이 (최대로), 임베딩 차원)\n",
        "#임베딩 전 : (batch, seq_len) 임베딩 후: (batch, seq_len, embed_dim)\n",
        "training_config = {\n",
        "    \"batch_size\": 16,\n",
        "    \"epochs\": 3,\n",
        "    \"lr\": 1e-4,\n",
        "    \"warmup_steps\": 50,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "\n",
        "    \"gradient_accumulate_steps\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\datasets--lemon-mint--korean_english_parallel_wiki_augmented_v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Generating train split: 100%|██████████| 503245/503245 [00:00<00:00, 507041.13 examples/s]\n",
            "Filter: 100%|██████████| 503245/503245 [00:02<00:00, 169336.74 examples/s]\n",
            "c:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-ko-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "c:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:176: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ],
      "source": [
        "#{\"english\": \"...\", \"korean\": \"...\"} 형태\n",
        "\n",
        "dataset = datasets.load_dataset(\"lemon-mint/korean_english_parallel_wiki_augmented_v1\",split=\"train\") #한영 병렬 코퍼스 다운로드\n",
        "\n",
        "#영어 한국어 모두 128자 초과 - 8192자 미만만 남김 \n",
        "dataset = dataset.filter(lambda x: len(x['english']) < 8192 and len(x['english']) > 128 and len(x['korean']) < 8192 and len(x['korean']) > 128) #길이제한\n",
        "\n",
        "valid_set = dataset.select(range(10000)) #검증용용\n",
        "train_set = dataset.select(range(10000, 110000)) #train 용\n",
        "\n",
        "#\"나는 너를 사랑해\" → [1203, 4421, 2891]\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\") #토크나이저 : 텍스트 -> 숫자 변환\n",
        "additional_special_tokens = {}\n",
        "#문장 시작, 종료, 패딩 토큰 추가가\n",
        "if tokenizer.pad_token is None:\n",
        "    additional_special_tokens[\"pad_token\"] = \"<pad>\"\n",
        "if tokenizer.eos_token is None:\n",
        "    additional_special_tokens[\"eos_token\"] = \"</s>\"\n",
        "if tokenizer.bos_token is None:\n",
        "    additional_special_tokens[\"bos_token\"] = \"<s>\"\n",
        "tokenizer.add_special_tokens(additional_special_tokens)\n",
        "\n",
        "#각 배치에서 텍스트 추출\n",
        "'''\n",
        "padding = True : 배치 내 가장 긴 문장에 맞춰 패딩\n",
        "truncation = True : 문장 길이 512 넘어가면 잘라냄\n",
        "return_tensors=\"pt\" : 파이토치 텐서로 변환\n",
        "max_length=512 : 문장 최대 길이 512\n",
        "pad_to_multiple_of=64 : 64의 배수로 패딩 - GPU 연산 최적화\n",
        "input_ids -> 토큰화된 문자를 숫자로 바꾼 값\n",
        "토크나이저 내부 로직이 알아서 패딩 자리를 0, 원래 토큰 자리에 1 넣어줌\n",
        "labels 는 loss 계산시 사용\n",
        "BOS 토큰 id = 0\n",
        "EOS 토큰 id = 2\n",
        "'''\n",
        "def collate_fn(batch):\n",
        "    english_corpus = [item[\"english\"] for item in batch]\n",
        "    korean_corpus = [item[\"korean\"] for item in batch]\n",
        "    english_corpus = tokenizer(english_corpus, padding=True, truncation=True, return_tensors=\"pt\", max_length=512, pad_to_multiple_of=64)\n",
        "    korean_corpus = tokenizer(korean_corpus, padding=True, truncation=True, return_tensors=\"pt\", max_length=512, pad_to_multiple_of=64)\n",
        "    labels = korean_corpus[\"input_ids\"].clone() #원본 복사\n",
        "    labels[korean_corpus['attention_mask'].eq(0)] = -100 #eq[0] 은 0 과 비교시 true false 반환하라는거고 -100 은 패딩 토큰 이렇게 바꿔 파이토치보고 무시하라는 뜻\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"encoder_input_ids\": english_corpus[\"input_ids\"],\n",
        "        \"encoder_attention_mask\": english_corpus[\"attention_mask\"],\n",
        "        \"decoder_input_ids\": korean_corpus[\"input_ids\"],\n",
        "        \"labels\": korean_corpus[\"input_ids\"],\n",
        "    } #배치 딕셔너리 형태 반환\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Seq2Seq 구조: Encoder(양방향 LSTM)가 입력 문장을 읽고, Decoder(단방향 LSTM)가 한 토큰씩 출력을 생성합니다. \n",
        "Attention은 Decoder가 매 스텝마다 Encoder의 어떤 부분에 집중할지 결정합니다.\n",
        "Input Feeding: 이전 타임스텝의 attention context를 현재 입력 임베딩에 concat해서 넣는 기법입니다. \n",
        "Decoder가 이전에 어디를 봤는지 알 수 있게 해줍니다.\n",
        "'''\n",
        "@dataclass\n",
        "class ModelConfig(object):\n",
        "    vocab_size: int = field(default=50000)\n",
        "    encoder_hidden_dim: int = field(default=512) # hidden dimention of encoder lstm\n",
        "    decoder_hidden_dim: int = field(default=512) # hidden dimention of decoder lstm\n",
        "    hidden_dim: int = field(default=512) # hidden dimention of other module like attention\n",
        "    embed_dim: int = field(default=512) # embedding dimention\n",
        "    pad_idx: int = field(default=0)\n",
        "    sos_idx: int = field(default=1)\n",
        "    eos_idx: int = field(default=2)\n",
        "    n_layers: int = field(default=1)\n",
        "    dropout: float = field(default=0.1)\n",
        "\n",
        "    attention_type:str = field(default=\"global\")\n",
        "    window_size: int = field(default=10)\n",
        "    sigma_ratio: float = field(default=2.0)\n",
        "\n",
        "    do_input_feeding: bool = field(default=True)\n",
        "\n",
        "'''\n",
        "Decoder 의 현재 hidden state 가 쿼리\n",
        "인코더의 모든 출력이 key, value 사용해 가중합\n",
        "'''\n",
        "\n",
        "class GlobalAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.query_proj = nn.Linear(config.decoder_hidden_dim, config.hidden_dim, bias=False) #입력 크기, 출력 크기, bias\n",
        "        self.key_proj = nn.Linear(config.encoder_hidden_dim * 2, config.hidden_dim, bias=False)\n",
        "        self.value_proj = nn.Linear(config.encoder_hidden_dim * 2, config.hidden_dim, bias=False)\n",
        "        self.output_proj = nn.Linear(config.hidden_dim, config.decoder_hidden_dim, bias=False)\n",
        "        \n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.scale = np.sqrt(config.hidden_dim)\n",
        "\n",
        "    def forward(self, decoder_hidden_query, encoder_outputs, encoder_attention_mask):\n",
        "        query = self.query_proj(decoder_hidden_query)\n",
        "        key = self.key_proj(encoder_outputs)\n",
        "        value = self.value_proj(encoder_outputs)\n",
        "        \n",
        "        # fill here for global attention forward\n",
        "        # shape hint:\n",
        "        # context: (batch, 1, hidden_dim)\n",
        "        ######\n",
        "\n",
        "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale #행렬 곱셈\n",
        "        attn_scores = attn_scores.masked_fill(encoder_attention_mask.unsqueeze(1) == 0, -1e9) #패딩 토큰 무시\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1) #소프트맥스 함수 적용\n",
        "        attn_weights = self.dropout(attn_weights) #드롭아웃 적용\n",
        "        context = torch.matmul(attn_weights, value) #컨텍스트 계산\n",
        "        \n",
        "        ######\n",
        "        output_context = self.output_proj(context)\n",
        "\n",
        "        return output_context\n",
        "\n",
        "class LocalAttention(GlobalAttention):\n",
        "    # 주변 윈도우만 고려\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__(config) \n",
        "        self.window_size = config.window_size #좌우로 몇개 볼것인가\n",
        "        self.location_proj_up = nn.Linear(config.decoder_hidden_dim, config.hidden_dim, bias=False)\n",
        "        self.location_proj_down = nn.Linear(config.hidden_dim, 1, bias=False)\n",
        "        self.sigma = self.window_size / config.sigma_ratio\n",
        "\n",
        "    def forward(self, decoder_hidden_query, encoder_outputs, encoder_attention_mask):\n",
        "        key, value, attn_mask, gaussian_penalty = self._gather_local_context(decoder_hidden_query, encoder_outputs, encoder_attention_mask)\n",
        "        query = self.query_proj(decoder_hidden_query)\n",
        "        key = self.key_proj(key)\n",
        "        value = self.value_proj(value)\n",
        "\n",
        "        # fill here for local attention forward\n",
        "        # shape hint:\n",
        "        # context: (batch, 1, hidden_dim)\n",
        "        ######\n",
        "        #attention score 계산\n",
        "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale\n",
        "        \n",
        "        #PAD masking\n",
        "        attn_scores = attn_scores.masked_fill(attn_mask == 0, -1e9)\n",
        "\n",
        "        #softmax 적용 \n",
        "        attn_weights = F.softmax(attn_scores, dim=-1) #dim -1 은 마지막 차원에 대해 softmax 하니 합쳐 1\n",
        "        \n",
        "        #gaussian penalty 적용\n",
        "        attn_weights = attn_weights * gaussian_penalty.unsqueeze(1) \n",
        "\n",
        "        #renormalize (가우시안 패널티 적용 후 합이 1 되도록)\n",
        "        attn_weights = attn_weights / (attn_weights.sum(dim=-1, keepdim=True) + 1e-10) #0 방지\n",
        "\n",
        "        #context vector 계산\n",
        "        context = torch.matmul(attn_weights, value)\n",
        "        \n",
        "        ######\n",
        "        output_context = self.output_proj(context)\n",
        "\n",
        "        return output_context\n",
        "\n",
        "    def _gather_local_context(self, decoder_hidden_query, encoder_outputs, encoder_attention_mask):\n",
        "        device = encoder_outputs.device #현재 디바이스 저장\n",
        "        src_len = encoder_attention_mask.sum(dim=-1).unsqueeze(-1) #각 샘플 실제 토큰 개수\n",
        "\n",
        "        # fill here for local context window\n",
        "        # shape hint:\n",
        "        # local_key: (batch, window_size * 2 + 1, hidden_dim)\n",
        "        # local_value: (batch, window_size * 2 + 1, hidden_dim)\n",
        "        # local_attn_mask: (batch, window_size * 2 + 1)\n",
        "        # gaussian_penalty: (batch, window_size * 2 + 1)\n",
        "        ######\n",
        "        \n",
        "        location_proj = torch.tanh(self.location_proj_up(decoder_hidden_query)) #위치 프로젝션\n",
        "        pt = torch.sigmoid(self.location_proj_down(location_proj))*src_len #실제 위치로 스케일링\n",
        "        pt = pt.squeeze(-1) #불필요한 차원 제거\n",
        "\n",
        "        batch_size, src_seq_len, hidden_dim = encoder_outputs.shape\n",
        "\n",
        "        window_positions = torch.arange(\n",
        "            -self.window_size, self.window_size + 1, device=device\n",
        "        ).float() #unsqueeze() 는 차원추가, 0,1 은 위치 -1 은 무조건 마지막 위치 broadcasting\n",
        "\n",
        "        positions = (pt.unsqueeze(-1) + window_positions.unsqueeze(0).unsqueeze(0)).long() #위치 계산, 정수 인덱싱 필요해서 long 타입\n",
        "        \n",
        "        #범위 지정\n",
        "        positions = positions.clamp(0, src_seq_len - 1) #0 과 src_seq_len - 1 사이로 제한\n",
        "        positions = positions.squeeze(1) #(batch, 2D+1)\n",
        "        positions_expanded = positions.unsqueeze(-1).expand(-1, -1, hidden_dim) \n",
        "\n",
        "        local_key = torch.gather(encoder_outputs, dim=1, index=positions_expanded)\n",
        "        local_value = local_key.clone()\n",
        "\n",
        "        #local attention mask 만들기\n",
        "        '''\n",
        "        torch.gather(input, dim, index) -> 특정 인덱스 위치 값들 수집\n",
        "        input : 원본 텐서\n",
        "        dim : 추출할 차원\n",
        "        index : 추출할 위치\n",
        "        '''\n",
        "        local_attn_mask = torch.gather(\n",
        "            encoder_attention_mask,\n",
        "            dim=1,\n",
        "            index=positions\n",
        "        )\n",
        "        window_positions_sq = window_positions.squeeze(0)\n",
        "\n",
        "        #가우시안 패널티 주기\n",
        "        gaussian_penalty = torch.exp(-(window_positions_sq**2) / (2 * self.sigma**2))\n",
        "        gaussian_penalty = gaussian_penalty.expand(batch_size, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
        "        ######\n",
        "\n",
        "        return local_key, local_value, local_attn_mask, gaussian_penalty\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    인풋 임베딩 : (batch, src_seq_len, embed_dim)\n",
        "    어텐션 마스크 : (batch, src_seq_len) \n",
        "    인코더 출력 : (batch, src_seq_len, hidden_dim)\n",
        "    인코더 은닉 상태 : (n_layers, batch, decoder_hidden_dim)\n",
        "    '''\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=config.embed_dim, #입력 특징 차원 적용\n",
        "            hidden_size=config.encoder_hidden_dim, #은닉 상태 차원 적용\n",
        "            num_layers=config.n_layers, #레이어 수\n",
        "            dropout=config.dropout if config.n_layers > 1 else 0, #드롭아웃\n",
        "            bidirectional=True,\n",
        "            batch_first=True #배치 첫 차원\n",
        "        )\n",
        "\n",
        "        self.h_dec_proj = nn.Linear(config.encoder_hidden_dim * 2, config.decoder_hidden_dim) #양뱡향 concat, decoder 지정\n",
        "        self.c_dec_proj = nn.Linear(config.encoder_hidden_dim * 2, config.decoder_hidden_dim)\n",
        "\n",
        "    def forward(self, input_embeds, attention_mask):\n",
        "\n",
        "        # Fill here for encoder forward\n",
        "        # shape hint\n",
        "        # input_embeds: (batch, src_seq_len, embed_dim)\n",
        "        # attention_mask: (batch, src_seq_len)\n",
        "        # encoder_output: (batch, src_seq_len, hidden_dim)\n",
        "        # h_enc: (n_layers, batch, decoder_hidden_dim)\n",
        "        # c_enc: (n_layers, batch, decoder_hidden_dim)\n",
        "        # hint for implementation\n",
        "        # 1. use nn.utils.rnn.pack_padded_sequence to packing inputs for rnn series, see https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "        #    failure to properly handle padding will result in a penalty.\n",
        "        # 2. lstm cell state and hidden state will be doubled because of bidirectional lstm.\n",
        "        #    decoder will be unidirectional for causal language modeling. \n",
        "        #    handle the hidden state and cell state to be same as decoder.\n",
        "        ######\n",
        "        lengths = attention_mask.sum(dim=1).cpu() #각 샘플 실제 길이 계산 \n",
        "\n",
        "        #패딩 제거\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(input_embeds, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        #Bidirectional LSTM \n",
        "        packed_output, (h_enc, c_enc) = self.encoder(packed_input)\n",
        "\n",
        "        encoder_output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        n_layers = self.config.n_layers\n",
        "        h_enc = h_enc.view(n_layers, 2, -1, self.config.decoder_hidden_dim)\n",
        "        c_enc = c_enc.view(n_layers, 2, -1, self.config.decoder_hidden_dim)\n",
        "\n",
        "        h_enc = torch.cat([h_enc[:,0,:,:], h_enc[:,1,:,:]], dim=-1)\n",
        "        c_enc = torch.cat([c_enc[:,0,:,:], c_enc[:,1,:,:]], dim=-1)\n",
        "\n",
        "        #projection\n",
        "        h_enc = self.h_dec_proj(h_enc)\n",
        "        c_enc = self.c_dec_proj(c_enc)\n",
        "        \n",
        "        ######\n",
        "\n",
        "        return encoder_output, (h_enc, c_enc)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=config.embed_dim + config.hidden_dim if config.do_input_feeding else config.embed_dim,\n",
        "            hidden_size=config.decoder_hidden_dim,\n",
        "            num_layers=config.n_layers,\n",
        "            dropout=config.dropout if config.n_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        match config.attention_type:\n",
        "            case \"local\":\n",
        "                self.attention = LocalAttention(config)\n",
        "            case \"global\":\n",
        "                self.attention = GlobalAttention(config)\n",
        "            case _:\n",
        "                raise ValueError(f\"Unknown attention type: {config.attention_type}\")\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, input_embeds, encoder_outputs, h_enc, c_enc, attention_mask):\n",
        "        decoder_output, (h_dec, c_dec) = self.decoder(input_embeds, (h_enc, c_enc))\n",
        "        attention_context = self.attention(decoder_output, encoder_outputs, attention_mask)\n",
        "        decoder_output = decoder_output + attention_context\n",
        "\n",
        "        return decoder_output, attention_context, (h_dec, c_dec)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim, padding_idx=config.pad_idx)\n",
        "        \n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "        \n",
        "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, encoder_input_ids, encoder_attention_mask, decoder_input_ids, labels=None, cache=None):\n",
        "        if cache is None:\n",
        "            encoder_input_embeds = self.embedding(encoder_input_ids)\n",
        "            encoder_outputs, (h_enc, c_enc) = self.encoder(encoder_input_embeds, encoder_attention_mask)\n",
        "\n",
        "            current_h_dec, current_c_dec = h_enc, c_enc\n",
        "            prev_attn_context = None\n",
        "        else:\n",
        "            encoder_outputs, current_h_dec, current_c_dec, prev_attn_context = cache\n",
        "\n",
        "        batch_size, tgt_len = decoder_input_ids.shape\n",
        "        decoder_input_embeds = self.embedding(decoder_input_ids)\n",
        "\n",
        "        if prev_attn_context is None:\n",
        "            prev_attn_context = torch.zeros((batch_size, 1, self.config.decoder_hidden_dim)).to(decoder_input_embeds)\n",
        "        \n",
        "        outputs = [] \n",
        "\n",
        "        for t in range(tgt_len):\n",
        "            # fill here for decoder forward\n",
        "            ######\n",
        "            #현재 타임스텝 임베딩 추출\n",
        "            current_embed = decoder_input_embeds[:, t:t+1, :]\n",
        "\n",
        "            #input feeding - 이전 attention context 현재 입력에 concat\n",
        "\n",
        "            if self.config.do_input_feeding:\n",
        "                current_embed = torch.cat([current_embed, prev_attn_context], dim=-1)\n",
        "            \n",
        "            decoder_output, attn_context, (h_dec, c_dec) = self.decoder(current_embed, encoder_outputs, current_h_dec, current_c_dec, encoder_attention_mask)\n",
        "            \n",
        "            current_h_dec = h_dec\n",
        "            current_c_dec = c_dec\n",
        "            prev_attn_context = attn_context\n",
        "            ######\n",
        "            outputs.append(decoder_output)\n",
        "            \n",
        "\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        \n",
        "        lm_logits = self.lm_head(outputs)\n",
        "    \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # for cross entropy loss\n",
        "            # loss must be scalar\n",
        "           \n",
        "            labels_for_loss = labels[:, 1:].contiguous()\n",
        "            lm_logits_for_loss = lm_logits[:, :-1, :].contiguous()\n",
        "            loss = F.cross_entropy(lm_logits_for_loss.view(-1, self.config.vocab_size), labels_for_loss.view(-1))\n",
        "           \n",
        "            return loss\n",
        "        else:\n",
        "            return lm_logits, (encoder_outputs, current_h_dec, current_c_dec, prev_attn_context)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        encoder_input_ids: torch.LongTensor,\n",
        "        encoder_attention_mask: torch.LongTensor,\n",
        "        max_new_tokens: int = 256,\n",
        "    ):\n",
        "        batch_size, _ = encoder_input_ids.shape\n",
        "        device = encoder_input_ids.device\n",
        "        eos = self.config.eos_idx\n",
        "\n",
        "        #생성되지 않은 문장 표시 (1=미완, 0=완성)\n",
        "        unfinish_flag = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "        cache = None\n",
        "\n",
        "        #SOS 토큰으로 시작\n",
        "        decoder_input_ids = torch.full((batch_size, 1), self.config.sos_idx, dtype=torch.long, device=device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # fill here for causal generation\n",
        "           ######\n",
        "        \n",
        "            lm_logits, cache = self.forward(encoder_input_ids, encoder_attention_mask, decoder_input_ids[:, -1:], cache=cache) #마지막 토큰만\n",
        "            next_token = lm_logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "\n",
        "            #unfinished_flag 가 0 인 문장은 패딩 토큰으로 대체\n",
        "            next_token = next_token * unfinish_flag.unsqueeze(-1) + self.config.pad_idx * (1 - unfinish_flag.unsqueeze(-1))\n",
        "\n",
        "            #생성 토큰 추가\n",
        "            decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=1)\n",
        "\n",
        "            unfinish_flag = unfinish_flag * (next_token.squeeze(-1) != eos).long()\n",
        "\n",
        "            if unfinish_flag.sum() == 0:\n",
        "                break\n",
        "            \n",
        "            ######\n",
        "        return decoder_input_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_dataset, valid_dataset, collate_fn, train_args, prefix):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=train_args[\"lr\"])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=train_args['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=train_args['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "    total_steps = len(train_dataloader) * train_args['epochs']\n",
        "\n",
        "    num_training_steps = train_args['epochs'] * (len(train_dataloader) // train_args['gradient_accumulate_steps'])\n",
        "    scheduler = transformers.get_scheduler(\n",
        "        name=\"cosine\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=train_args['warmup_steps'],\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    best_loss = 987654321\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output_path = os.path.join(\"output\", prefix)\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    with open(os.path.join(output_path, \"train_args.json\"), \"w\") as f:\n",
        "        json.dump(train_args, f)\n",
        "\n",
        "    pbar = tqdm.tqdm(total=total_steps, desc=\"training\")\n",
        "    for epoch in range(train_args['epochs']):\n",
        "        pbar.set_description(f\"Epoch {epoch+1}/{train_args['epochs']}\")\n",
        "        move_avg_loss = []\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            batch = {k:v.to(train_args['device']) if isinstance(v,torch.Tensor) else v for k,v in batch.items()}\n",
        "\n",
        "            loss = model(**batch)\n",
        "            loss = loss / train_args['gradient_accumulate_steps']\n",
        "            if loss.size() != torch.Size([]):\n",
        "                loss = loss.mean()\n",
        "            loss.backward()\n",
        "            \n",
        "            if (i+1) % train_args['gradient_accumulate_steps'] == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "\n",
        "            move_avg_loss.append(loss.item()) \n",
        "            if len(move_avg_loss) > 100: move_avg_loss.pop(0)\n",
        "            pbar.set_postfix_str(f\"loss: {sum(move_avg_loss)/len(move_avg_loss):.04f} lr: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "            pbar.update(1)\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            eval_loss = 0\n",
        "            for i, batch in enumerate(valid_dataloader):\n",
        "                batch = {k:v.to(train_args['device']) if isinstance(v,torch.Tensor) else v for k,v in batch.items()}\n",
        "                loss_val = model(**batch)\n",
        "                if loss_val.size() != torch.Size([]):\n",
        "                    loss_val = loss_val.mean()\n",
        "                eval_loss += loss_val.item()\n",
        "                pbar.set_postfix_str(f\"val_loss: {eval_loss / (i+1):.04f}\")\n",
        "        eval_loss /= len(valid_dataloader)\n",
        "        pbar.write(f\"Validation Loss: {eval_loss:.04f}\")\n",
        "\n",
        "        if eval_loss < best_loss:\n",
        "            best_loss = eval_loss\n",
        "            \n",
        "            torch.save(model.state_dict(), os.path.join(output_path,\"best_model.pth\"))\n",
        "            pbar.write(f\"Model Saved best loss: {best_loss:.04f}\")\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "def evaluate(model, dataset, tokenizer, collate_fn, train_args):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=train_args['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "    \n",
        "    answers = []\n",
        "    predicts = []\n",
        "    for i, batch in enumerate(tqdm.tqdm(dataloader, desc=\"Evaluating\")):\n",
        "        batch = {k:v.to(train_args['device']) if isinstance(v,torch.Tensor) else v for k,v in batch.items()}\n",
        "        gen_output = model.generate(\n",
        "            encoder_input_ids=batch[\"encoder_input_ids\"],\n",
        "            encoder_attention_mask=batch[\"encoder_attention_mask\"],\n",
        "            max_new_tokens=512\n",
        "        )\n",
        "        pred = tokenizer.batch_decode(gen_output, skip_special_tokens=True)\n",
        "        ans = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
        "        answers.extend(ans)\n",
        "        predicts.extend(pred)\n",
        "    \n",
        "    bleu = load(\"bleu\")\n",
        "    result = bleu.compute(predictions=predicts, references=answers)\n",
        "    print(f\"BLEU: {result['bleu']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (embedding): Embedding(65002, 512, padding_idx=65000)\n",
            "  (encoder): Encoder(\n",
            "    (encoder): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "    (h_dec_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (c_dec_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (decoder): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (attention): GlobalAttention(\n",
            "      (query_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (key_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
            "      (value_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
            "      (output_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=65002, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A"
          ]
        }
      ],
      "source": [
        "config = ModelConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    pad_idx=tokenizer.pad_token_id,\n",
        "    sos_idx=tokenizer.bos_token_id,\n",
        "    eos_idx=tokenizer.eos_token_id,\n",
        "    n_layers=2,\n",
        "    dropout=0.1,\n",
        "\n",
        "    attention_type=\"global\",\n",
        "    do_input_feeding=False,\n",
        ")\n",
        "\n",
        "model = Seq2Seq(config).to(training_config[\"device\"])\n",
        "model = model.to(torch.bfloat16)\n",
        "print(model)\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_set,\n",
        "    valid_set,\n",
        "    collate_fn,\n",
        "    training_config,\n",
        "    prefix=\"seq2seq_global_attention_no_input_feeding\"\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(\"output\", \"seq2seq_global_attention_no_input_feeding\", \"best_model.pth\")))\n",
        "evaluate(\n",
        "    model,\n",
        "    valid_set,\n",
        "    tokenizer,\n",
        "    collate_fn,\n",
        "    training_config\n",
        ")\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (embedding): Embedding(65002, 512, padding_idx=65000)\n",
            "  (encoder): Encoder(\n",
            "    (encoder): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "    (h_dec_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (c_dec_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (decoder): LSTM(1024, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (attention): GlobalAttention(\n",
            "      (query_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (key_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
            "      (value_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
            "      (output_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=65002, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "ename": "InductorError",
          "evalue": "RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mInductorError\u001b[39m                             Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m model.compile()\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseq2seq_global_attention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m model.load_state_dict(torch.load(os.path.join(\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mseq2seq_global_attention\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbest_model.pth\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     27\u001b[39m evaluate(\n\u001b[32m     28\u001b[39m     model,\n\u001b[32m     29\u001b[39m     valid_set,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     training_config\n\u001b[32m     33\u001b[39m )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_dataset, valid_dataset, collate_fn, train_args, prefix)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[32m     31\u001b[39m     batch = {k:v.to(train_args[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch.items()}\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     loss = loss / train_args[\u001b[33m'\u001b[39m\u001b[33mgradient_accumulate_steps\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss.size() != torch.Size([]):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1774\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1774\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compiled_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1776\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:967\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    963\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__cause__\u001b[39;00m  \u001b[38;5;66;03m# User compiler error\u001b[39;00m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    965\u001b[39m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[32m    966\u001b[39m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    970\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1019\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe()).with_traceback(\n\u001b[32m   1020\u001b[39m         e.__traceback__\n\u001b[32m   1021\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1023\u001b[39m     TritonBundler.end_compile()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1003\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m   1001\u001b[39m TritonBundler.begin_compile()\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m     mb_compiled_graph = \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1007\u001b[39m     mb_compiled_graph._time_taken_ns = time.time_ns() - start_time\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1766\u001b[39m, in \u001b[36mfx_codegen_and_compile\u001b[39m\u001b[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[39m\n\u001b[32m   1763\u001b[39m     scheme = _ProgressiveFxCompile(fast_scheme, scheme, progression_configs)\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# pyrefly: ignore [unbound-name]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1537\u001b[39m, in \u001b[36m_InProcessFxCompile.codegen_and_compile\u001b[39m\u001b[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[39m\n\u001b[32m   1519\u001b[39m         compiled_fn = AotCodeCompiler.compile(\n\u001b[32m   1520\u001b[39m             graph,\n\u001b[32m   1521\u001b[39m             wrapper_code.value,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1534\u001b[39m             ],\n\u001b[32m   1535\u001b[39m         )\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1537\u001b[39m     compiled_module = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1538\u001b[39m     compiled_fn = compiled_module.call\n\u001b[32m   1539\u001b[39m     compiled_fn_runner = \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1540\u001b[39m         compiled_module, \u001b[33m\"\u001b[39m\u001b[33mrunner\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1541\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\graph.py:2416\u001b[39m, in \u001b[36mGraphLowering.compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2410\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   2411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraphLowering.compile_to_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2412\u001b[39m         phase_name=\u001b[33m\"\u001b[39m\u001b[33mcode_gen\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2413\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2414\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2415\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m2416\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\graph.py:2422\u001b[39m, in \u001b[36mGraphLowering._compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2418\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2419\u001b[39m     \u001b[38;5;66;03m# If we're here, we don't have to worry about the kernel code, which is only\u001b[39;00m\n\u001b[32m   2420\u001b[39m     \u001b[38;5;66;03m# returned separately in AOTInductor mode.\u001b[39;00m\n\u001b[32m   2421\u001b[39m     wrapper_code, _ = (\n\u001b[32m-> \u001b[39m\u001b[32m2422\u001b[39m         \u001b[38;5;28mself\u001b[39m.codegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2423\u001b[39m     )\n\u001b[32m   2425\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n\u001b[32m   2426\u001b[39m         mod = \u001b[38;5;28mself\u001b[39m._compile_to_module_lines(wrapper_code)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\graph.py:2358\u001b[39m, in \u001b[36mGraphLowering.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2355\u001b[39m V.debug.draw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m.orig_gm, \u001b[38;5;28mself\u001b[39m.scheduler.nodes)\n\u001b[32m   2357\u001b[39m \u001b[38;5;28mself\u001b[39m.wrapper_code.push_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2358\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2360\u001b[39m log.debug(\n\u001b[32m   2361\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished codegen for all nodes. The list of kernel names available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2362\u001b[39m     V.graph.all_codegen_kernel_names,\n\u001b[32m   2363\u001b[39m )\n\u001b[32m   2365\u001b[39m result = \u001b[38;5;28mself\u001b[39m.wrapper_code.generate(\u001b[38;5;28mself\u001b[39m.is_inference)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:6019\u001b[39m, in \u001b[36mScheduler.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   6016\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcodegen\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   6017\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mScheduler.codegen\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   6018\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m6019\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codegen_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6020\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m torch._inductor.config.graph_partition\n\u001b[32m   6021\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._codegen(\u001b[38;5;28mself\u001b[39m.nodes)\n\u001b[32m   6022\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:6161\u001b[39m, in \u001b[36mScheduler._codegen_partitions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   6156\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(partition) >= \u001b[32m1\u001b[39m, (\n\u001b[32m   6157\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEach partition must have at least one node but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(partition)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   6158\u001b[39m )\n\u001b[32m   6160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m signature.skip_cudagraph:\n\u001b[32m-> \u001b[39m\u001b[32m6161\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6163\u001b[39m     \u001b[38;5;28mself\u001b[39m._codegen_partition_wrapper(partition, signature)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:6266\u001b[39m, in \u001b[36mScheduler._codegen\u001b[39m\u001b[34m(self, nodes)\u001b[39m\n\u001b[32m   6263\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_backend(device).codegen_mix_order_reduction(node)\n\u001b[32m   6264\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[32m   6265\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [unbound-name]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6266\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6267\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6268\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NopKernelSchedulerNode)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:5295\u001b[39m, in \u001b[36mCppScheduling.codegen_node\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m   5293\u001b[39m nodes: \u001b[38;5;28mlist\u001b[39m[SchedulerNode] = node.get_nodes()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m   5294\u001b[39m nodes = \u001b[38;5;28mself\u001b[39m.try_loop_split(nodes)\n\u001b[32m-> \u001b[39m\u001b[32m5295\u001b[39m cpp_kernel_proxy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_proxy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5296\u001b[39m cpp_kernel_proxy.codegen_nodes(nodes)\n\u001b[32m   5297\u001b[39m kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4018\u001b[39m, in \u001b[36mCppKernelProxy.__init__\u001b[39m\u001b[34m(self, kernel_group)\u001b[39m\n\u001b[32m   4016\u001b[39m \u001b[38;5;28mself\u001b[39m.loop_nest = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4017\u001b[39m \u001b[38;5;28mself\u001b[39m.call_ranges = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4018\u001b[39m \u001b[38;5;28mself\u001b[39m.picked_vec_isa: cpu_vec_isa.VecISA = \u001b[43mcpu_vec_isa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpick_vec_isa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4019\u001b[39m \u001b[38;5;28mself\u001b[39m.kernels: \u001b[38;5;28mlist\u001b[39m[CppKernel] = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:551\u001b[39m, in \u001b[36mpick_vec_isa\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode() \u001b[38;5;129;01mand\u001b[39;00m (platform.machine() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mx86_64\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAMD64\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m VecAVX2()\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m _valid_vec_isa_list: \u001b[38;5;28mlist\u001b[39m[VecISA] = \u001b[43mvalid_vec_isa_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _valid_vec_isa_list:\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_vec_isa\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:538\u001b[39m, in \u001b[36mvalid_vec_isa_list\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    535\u001b[39m \u001b[33;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[32m    536\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    537\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m     \u001b[43misa_list\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m        \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupported_vec_isa_list\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_cpu_supported_x86_isa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43misa\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:538\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    535\u001b[39m \u001b[33;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[32m    536\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    537\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m     isa_list.extend(\n\u001b[32m    539\u001b[39m         isa\n\u001b[32m    540\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m isa \u001b[38;5;129;01min\u001b[39;00m supported_vec_isa_list\n\u001b[32m    541\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(flag \u001b[38;5;129;01min\u001b[39;00m _cpu_supported_x86_isa \u001b[38;5;28;01mfor\u001b[39;00m flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(isa).split()) \u001b[38;5;129;01mand\u001b[39;00m isa\n\u001b[32m    542\u001b[39m     )\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:144\u001b[39m, in \u001b[36mVecISA.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__bool__impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvec_isa_ok\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:154\u001b[39m, in \u001b[36mVecISA.__bool__impl\u001b[39m\u001b[34m(self, vec_isa_ok)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode():\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVecISA\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_avx_code\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:104\u001b[39m, in \u001b[36mVecISA.check_build\u001b[39m\u001b[34m(self, code)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lock_dir, LOCK_TIMEOUT, write\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     96\u001b[39m     CppBuilder,\n\u001b[32m     97\u001b[39m     CppTorchOptions,\n\u001b[32m     98\u001b[39m     normalize_path_separator,\n\u001b[32m     99\u001b[39m )\n\u001b[32m    101\u001b[39m key, input_path = write(\n\u001b[32m    102\u001b[39m     code,\n\u001b[32m    103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcpp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     extra=\u001b[43m_get_isa_dry_compile_fingerprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arch_flags\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    105\u001b[39m )\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_filelock\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileLock\n\u001b[32m    108\u001b[39m lock_dir = get_lock_dir()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:30\u001b[39m, in \u001b[36m_get_isa_dry_compile_fingerprint\u001b[39m\u001b[34m(isa_flags)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_isa_dry_compile_fingerprint\u001b[39m(isa_flags: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# ISA dry compile will cost about 1 sec time each startup time.\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Please check the issue: https://github.com/pytorch/pytorch/issues/100378\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# and generated them to output binary hash path.\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# It would optimize and skip compile existing binary.\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compiler_version_info, get_cpp_compiler\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     compiler_info = get_compiler_version_info(\u001b[43mget_cpp_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     31\u001b[39m     torch_version = torch.__version__\n\u001b[32m     32\u001b[39m     fingerprint = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misa_flags\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:383\u001b[39m, in \u001b[36mget_cpp_compiler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    381\u001b[39m     compiler = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mCXX\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    382\u001b[39m     compiler = normalize_path_separator(compiler)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[43mcheck_compiler_exist_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m     check_msvc_cl_language_id(compiler)\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:142\u001b[39m, in \u001b[36mcheck_compiler_exist_windows\u001b[39m\u001b[34m(compiler)\u001b[39m\n\u001b[32m    140\u001b[39m     subprocess.check_output([compiler, \u001b[33m\"\u001b[39m\u001b[33m/help\u001b[39m\u001b[33m\"\u001b[39m], stderr=subprocess.STDOUT)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompiler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not found.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.SubprocessError:\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# Expected that some compiler(clang, clang++) is exist, but they not support `/help` args.\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[31mInductorError\u001b[39m: RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
          ]
        }
      ],
      "source": [
        "config = ModelConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    pad_idx=tokenizer.pad_token_id,\n",
        "    sos_idx=tokenizer.bos_token_id,\n",
        "    eos_idx=tokenizer.eos_token_id,\n",
        "    n_layers=2,\n",
        "    dropout=0.1,\n",
        "\n",
        "    attention_type=\"global\",\n",
        ")\n",
        "\n",
        "model = Seq2Seq(config).to(training_config[\"device\"])\n",
        "model = model.to(torch.bfloat16)\n",
        "print(model)\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_set,\n",
        "    valid_set,\n",
        "    collate_fn,\n",
        "    training_config,\n",
        "    prefix=\"seq2seq_global_attention\"\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(\"output\", \"seq2seq_global_attention\", \"best_model.pth\")))\n",
        "evaluate(\n",
        "    model,\n",
        "    valid_set,\n",
        "    tokenizer,\n",
        "    collate_fn,\n",
        "    training_config\n",
        ")\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (embedding): Embedding(65002, 512, padding_idx=65000)\n",
            "  (encoder): Encoder(\n",
            "    (encoder): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "    (h_dec_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (c_dec_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (decoder): LSTM(1024, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (attention): LocalAttention(\n",
            "      (query_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (key_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
            "      (value_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
            "      (output_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (location_proj_up): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (location_proj_down): Linear(in_features=512, out_features=1, bias=False)\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=65002, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A"
          ]
        },
        {
          "ename": "InductorError",
          "evalue": "RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mInductorError\u001b[39m                             Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m model.compile()\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseq2seq_local_attention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m model.load_state_dict(torch.load(os.path.join(\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mseq2seq_local_attention\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbest_model.pth\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     27\u001b[39m evaluate(\n\u001b[32m     28\u001b[39m     model,\n\u001b[32m     29\u001b[39m     valid_set,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     training_config\n\u001b[32m     33\u001b[39m )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_dataset, valid_dataset, collate_fn, train_args, prefix)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[32m     31\u001b[39m     batch = {k:v.to(train_args[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch.items()}\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     loss = loss / train_args[\u001b[33m'\u001b[39m\u001b[33mgradient_accumulate_steps\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss.size() != torch.Size([]):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1774\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1774\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compiled_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1776\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:967\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    963\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__cause__\u001b[39;00m  \u001b[38;5;66;03m# User compiler error\u001b[39;00m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    965\u001b[39m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[32m    966\u001b[39m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    970\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1019\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe()).with_traceback(\n\u001b[32m   1020\u001b[39m         e.__traceback__\n\u001b[32m   1021\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1023\u001b[39m     TritonBundler.end_compile()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1003\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m   1001\u001b[39m TritonBundler.begin_compile()\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m     mb_compiled_graph = \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1007\u001b[39m     mb_compiled_graph._time_taken_ns = time.time_ns() - start_time\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1766\u001b[39m, in \u001b[36mfx_codegen_and_compile\u001b[39m\u001b[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[39m\n\u001b[32m   1763\u001b[39m     scheme = _ProgressiveFxCompile(fast_scheme, scheme, progression_configs)\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# pyrefly: ignore [unbound-name]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1537\u001b[39m, in \u001b[36m_InProcessFxCompile.codegen_and_compile\u001b[39m\u001b[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[39m\n\u001b[32m   1519\u001b[39m         compiled_fn = AotCodeCompiler.compile(\n\u001b[32m   1520\u001b[39m             graph,\n\u001b[32m   1521\u001b[39m             wrapper_code.value,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1534\u001b[39m             ],\n\u001b[32m   1535\u001b[39m         )\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1537\u001b[39m     compiled_module = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1538\u001b[39m     compiled_fn = compiled_module.call\n\u001b[32m   1539\u001b[39m     compiled_fn_runner = \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1540\u001b[39m         compiled_module, \u001b[33m\"\u001b[39m\u001b[33mrunner\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1541\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\graph.py:2416\u001b[39m, in \u001b[36mGraphLowering.compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2410\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   2411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraphLowering.compile_to_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2412\u001b[39m         phase_name=\u001b[33m\"\u001b[39m\u001b[33mcode_gen\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2413\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2414\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2415\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m2416\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\graph.py:2422\u001b[39m, in \u001b[36mGraphLowering._compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2418\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2419\u001b[39m     \u001b[38;5;66;03m# If we're here, we don't have to worry about the kernel code, which is only\u001b[39;00m\n\u001b[32m   2420\u001b[39m     \u001b[38;5;66;03m# returned separately in AOTInductor mode.\u001b[39;00m\n\u001b[32m   2421\u001b[39m     wrapper_code, _ = (\n\u001b[32m-> \u001b[39m\u001b[32m2422\u001b[39m         \u001b[38;5;28mself\u001b[39m.codegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2423\u001b[39m     )\n\u001b[32m   2425\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n\u001b[32m   2426\u001b[39m         mod = \u001b[38;5;28mself\u001b[39m._compile_to_module_lines(wrapper_code)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\graph.py:2358\u001b[39m, in \u001b[36mGraphLowering.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2355\u001b[39m V.debug.draw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m.orig_gm, \u001b[38;5;28mself\u001b[39m.scheduler.nodes)\n\u001b[32m   2357\u001b[39m \u001b[38;5;28mself\u001b[39m.wrapper_code.push_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2358\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2360\u001b[39m log.debug(\n\u001b[32m   2361\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished codegen for all nodes. The list of kernel names available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2362\u001b[39m     V.graph.all_codegen_kernel_names,\n\u001b[32m   2363\u001b[39m )\n\u001b[32m   2365\u001b[39m result = \u001b[38;5;28mself\u001b[39m.wrapper_code.generate(\u001b[38;5;28mself\u001b[39m.is_inference)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:6019\u001b[39m, in \u001b[36mScheduler.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   6016\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcodegen\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   6017\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mScheduler.codegen\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   6018\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m6019\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codegen_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6020\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m torch._inductor.config.graph_partition\n\u001b[32m   6021\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._codegen(\u001b[38;5;28mself\u001b[39m.nodes)\n\u001b[32m   6022\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:6161\u001b[39m, in \u001b[36mScheduler._codegen_partitions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   6156\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(partition) >= \u001b[32m1\u001b[39m, (\n\u001b[32m   6157\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEach partition must have at least one node but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(partition)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   6158\u001b[39m )\n\u001b[32m   6160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m signature.skip_cudagraph:\n\u001b[32m-> \u001b[39m\u001b[32m6161\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6163\u001b[39m     \u001b[38;5;28mself\u001b[39m._codegen_partition_wrapper(partition, signature)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:6266\u001b[39m, in \u001b[36mScheduler._codegen\u001b[39m\u001b[34m(self, nodes)\u001b[39m\n\u001b[32m   6263\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_backend(device).codegen_mix_order_reduction(node)\n\u001b[32m   6264\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[32m   6265\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [unbound-name]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6266\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6267\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6268\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NopKernelSchedulerNode)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:5295\u001b[39m, in \u001b[36mCppScheduling.codegen_node\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m   5293\u001b[39m nodes: \u001b[38;5;28mlist\u001b[39m[SchedulerNode] = node.get_nodes()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m   5294\u001b[39m nodes = \u001b[38;5;28mself\u001b[39m.try_loop_split(nodes)\n\u001b[32m-> \u001b[39m\u001b[32m5295\u001b[39m cpp_kernel_proxy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_proxy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5296\u001b[39m cpp_kernel_proxy.codegen_nodes(nodes)\n\u001b[32m   5297\u001b[39m kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4018\u001b[39m, in \u001b[36mCppKernelProxy.__init__\u001b[39m\u001b[34m(self, kernel_group)\u001b[39m\n\u001b[32m   4016\u001b[39m \u001b[38;5;28mself\u001b[39m.loop_nest = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4017\u001b[39m \u001b[38;5;28mself\u001b[39m.call_ranges = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4018\u001b[39m \u001b[38;5;28mself\u001b[39m.picked_vec_isa: cpu_vec_isa.VecISA = \u001b[43mcpu_vec_isa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpick_vec_isa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4019\u001b[39m \u001b[38;5;28mself\u001b[39m.kernels: \u001b[38;5;28mlist\u001b[39m[CppKernel] = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:551\u001b[39m, in \u001b[36mpick_vec_isa\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode() \u001b[38;5;129;01mand\u001b[39;00m (platform.machine() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mx86_64\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAMD64\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m VecAVX2()\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m _valid_vec_isa_list: \u001b[38;5;28mlist\u001b[39m[VecISA] = \u001b[43mvalid_vec_isa_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _valid_vec_isa_list:\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_vec_isa\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:538\u001b[39m, in \u001b[36mvalid_vec_isa_list\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    535\u001b[39m \u001b[33;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[32m    536\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    537\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m     \u001b[43misa_list\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m        \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupported_vec_isa_list\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_cpu_supported_x86_isa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43misa\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:538\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    535\u001b[39m \u001b[33;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[32m    536\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    537\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m     isa_list.extend(\n\u001b[32m    539\u001b[39m         isa\n\u001b[32m    540\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m isa \u001b[38;5;129;01min\u001b[39;00m supported_vec_isa_list\n\u001b[32m    541\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(flag \u001b[38;5;129;01min\u001b[39;00m _cpu_supported_x86_isa \u001b[38;5;28;01mfor\u001b[39;00m flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(isa).split()) \u001b[38;5;129;01mand\u001b[39;00m isa\n\u001b[32m    542\u001b[39m     )\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:144\u001b[39m, in \u001b[36mVecISA.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__bool__impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvec_isa_ok\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:154\u001b[39m, in \u001b[36mVecISA.__bool__impl\u001b[39m\u001b[34m(self, vec_isa_ok)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode():\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVecISA\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_avx_code\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:104\u001b[39m, in \u001b[36mVecISA.check_build\u001b[39m\u001b[34m(self, code)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lock_dir, LOCK_TIMEOUT, write\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     96\u001b[39m     CppBuilder,\n\u001b[32m     97\u001b[39m     CppTorchOptions,\n\u001b[32m     98\u001b[39m     normalize_path_separator,\n\u001b[32m     99\u001b[39m )\n\u001b[32m    101\u001b[39m key, input_path = write(\n\u001b[32m    102\u001b[39m     code,\n\u001b[32m    103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcpp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     extra=\u001b[43m_get_isa_dry_compile_fingerprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arch_flags\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    105\u001b[39m )\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_filelock\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileLock\n\u001b[32m    108\u001b[39m lock_dir = get_lock_dir()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:30\u001b[39m, in \u001b[36m_get_isa_dry_compile_fingerprint\u001b[39m\u001b[34m(isa_flags)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_isa_dry_compile_fingerprint\u001b[39m(isa_flags: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# ISA dry compile will cost about 1 sec time each startup time.\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Please check the issue: https://github.com/pytorch/pytorch/issues/100378\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# and generated them to output binary hash path.\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# It would optimize and skip compile existing binary.\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compiler_version_info, get_cpp_compiler\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     compiler_info = get_compiler_version_info(\u001b[43mget_cpp_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     31\u001b[39m     torch_version = torch.__version__\n\u001b[32m     32\u001b[39m     fingerprint = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misa_flags\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:383\u001b[39m, in \u001b[36mget_cpp_compiler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    381\u001b[39m     compiler = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mCXX\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    382\u001b[39m     compiler = normalize_path_separator(compiler)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[43mcheck_compiler_exist_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m     check_msvc_cl_language_id(compiler)\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Downloads\\a2\\venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:142\u001b[39m, in \u001b[36mcheck_compiler_exist_windows\u001b[39m\u001b[34m(compiler)\u001b[39m\n\u001b[32m    140\u001b[39m     subprocess.check_output([compiler, \u001b[33m\"\u001b[39m\u001b[33m/help\u001b[39m\u001b[33m\"\u001b[39m], stderr=subprocess.STDOUT)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompiler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not found.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.SubprocessError:\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# Expected that some compiler(clang, clang++) is exist, but they not support `/help` args.\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[31mInductorError\u001b[39m: RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
          ]
        }
      ],
      "source": [
        "config = ModelConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    pad_idx=tokenizer.pad_token_id,\n",
        "    sos_idx=tokenizer.bos_token_id,\n",
        "    eos_idx=tokenizer.eos_token_id,\n",
        "    n_layers=2,\n",
        "    dropout=0.1,\n",
        "\n",
        "    attention_type=\"local\",\n",
        ")\n",
        "\n",
        "model = Seq2Seq(config).to(training_config[\"device\"])\n",
        "model = model.to(torch.bfloat16)\n",
        "print(model)\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_set,\n",
        "    valid_set,\n",
        "    collate_fn,\n",
        "    training_config,\n",
        "    prefix=\"seq2seq_local_attention\"\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(\"output\", \"seq2seq_local_attention\", \"best_model.pth\")))\n",
        "evaluate(\n",
        "    model,\n",
        "    valid_set,\n",
        "    tokenizer,\n",
        "    collate_fn,\n",
        "    training_config\n",
        ")\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
