{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "import datasets\n",
    "import tqdm\n",
    "import math\n",
    "from functools import partial\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "import sentencepiece\n",
    "import shutil\n",
    "import copy\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_training_args = {\n",
    "    \"batch_size\": 256,  # CPU용으로 축소 (32768 → 256)\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 1e-3,\n",
    "    \"device\": \"cpu\",  # RTX 5070 CUDA 호환성 문제로 CPU 사용\n",
    "    \"embedding_dim\": 384,\n",
    "\n",
    "    \"vocab_size\": 50000,\n",
    "    \"min_freq\": 5,\n",
    "\n",
    "    \"gradient_accumulate_steps\": 1,\n",
    "}\n",
    "\n",
    "tokenizer_args = {\n",
    "    \"vocab_size\": 50000,\n",
    "    \"min_freq\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "토크나이저 : 텍스트를 모델이 이해할 수 있게 토큰 단위로 나누는 과정\n",
    "BPE : 자주 등장하는 문자 조합을 하나의 토큰으로 묶는다\n",
    "special_tokens 로는 전체 어휘 구성시 사용\n",
    "additional_special_tokens 는 추가 토큰만 사용시\n",
    "'''\n",
    "import re\n",
    "class Tokenizer:\n",
    "    def __init__(self, \n",
    "                 max_vocab_size=10000, #최대 어휘 크기 (단어 개수 제한)\n",
    "                 special_tokens=[], #추가 특수 토큰 리스트\n",
    "                 pad_token=\"<|PAD|>\", #패딩 (시퀀스 길이 맞춤용)\n",
    "                 unk_token=\"<|UNK|>\", #미등록 단어\n",
    "                 bos_token=\"<|BOS|>\", #문장 시작\n",
    "                 eos_token=\"<|EOS|>\"): #문장 종료\n",
    "        self.max_vocab_size = max_vocab_size #최대 어휘 크기 저장\n",
    "        self.tokenizer = None #토크나이저 객체 (하위 클래스에서 사용)\n",
    "\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_token_id = 1\n",
    "        self.bos_token = bos_token\n",
    "        self.bos_token_id = 2\n",
    "        self.eos_token = eos_token\n",
    "        self.eos_token_id = 3\n",
    "\n",
    "        self.special_tokens = [self.pad_token, self.unk_token, self.bos_token, self.eos_token] + special_tokens\n",
    "        self.additional_special_tokens = special_tokens\n",
    "    \n",
    "    def save(self, path): #저장경로 디렉터리 생성\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        with open(os.path.join(path,\"mics.json\"), \"w\") as f: #mics.json 생성\n",
    "            json.dump({ #메타 정보 json 으로 지정 후 저장\n",
    "                \"max_vocab_size\": self.max_vocab_size,\n",
    "                \"special_tokens\": self.special_tokens,\n",
    "                \"additional_special_tokens\": self.additional_special_tokens,\n",
    "            }, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        with open(os.path.join(path,\"mics.json\"), \"r\") as f: #디렉토리에서 load\n",
    "            mics = json.load(f)\n",
    "            self.max_vocab_size = mics[\"max_vocab_size\"]\n",
    "            self.special_tokens = mics[\"special_tokens\"]\n",
    "            self.additional_special_tokens = mics[\"additional_special_tokens\"] #읽어오기\n",
    "\n",
    "class WitespaceTokenizer(Tokenizer): #기본 상속\n",
    "    def __init__(self, min_count=5, max_vocab_size=10000, special_tokens=[]):\n",
    "        super().__init__(max_vocab_size, special_tokens) #부모 클래스 초기화\n",
    "        self.min_count = min_count #최소 등장 횟수 제한\n",
    "        self.vocab = {} #어휘 사전 초기화\n",
    "\n",
    "        assert len(special_tokens) == len(set(special_tokens)), \"Duplicate special tokens are not allowed.\" #중복 토큰 확인 만약 중복 있으면 오류 메시지\n",
    "        assert len(special_tokens) < max_vocab_size, \"Special tokens exceed max vocab size.\" #토큰 개수 제한\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab) #어휘 사전 크기 반환\n",
    "\n",
    "    def normalize(self, text):\n",
    "        import re  # multiprocessing 호환을 위해 함수 내부에서 import\n",
    "        return re.sub(r'[^\\w\\s]', '', text).lower() #문자열 정규화\n",
    "\n",
    "    def fit(self, dataset): #어휘 사전 구축\n",
    "        token_counter = Counter() #단어 빈도수 카운터\n",
    "        for text in tqdm.tqdm(dataset, desc=\"WitespaceTokenizer fitting...\"): #진행상황 표시시\n",
    "            tokens = self.normalize(text).split() #정규화 후 공백 분리\n",
    "            token_counter.update(tokens) #토큰 빈도수 업데이트\n",
    "        \n",
    "        vocab = [(word,count) for word, count in token_counter.items() if count >= self.min_count] #vocab 구축\n",
    "        vocab = sorted(vocab, key=lambda x: -x[1]) #빈도수 내림차순 정렬\n",
    "        vocab = vocab[:(self.max_vocab_size - len(self.special_tokens))] #특수 토큰 개수 빼서 최대 어휘 크기 맞춤\n",
    "        vocab = [word for word, _ in vocab] #단어 리스트로 변환\n",
    "        vocab = self.special_tokens + vocab #특수 토큰 추가\n",
    "        \n",
    "        self.sample_weights = [] #샘플 가중치 초기화\n",
    "        for i, word in enumerate(vocab): #리스트를 순회하면서 인덱스와 값 동시 반환\n",
    "            self.vocab[word] = i\n",
    "            self.sample_weights.append(math.log(token_counter[word]) if word in token_counter else -987654321) #빈도수 로그 취해서 가중치 계산\n",
    "\n",
    "    def tokenize(self, text, add_special_tokens=False): #토큰화\n",
    "        tokens = self.normalize(text).split()\n",
    "        if add_special_tokens:\n",
    "            tokens = [\"<|BOS|>\"] + tokens + [\"<|EOS|>\"]\n",
    "        return [self.vocab.get(token, self.vocab[\"<|UNK|>\"]) for token in tokens] #get(키, 기본값) 키가 없으면 기본값 반환 & 모든 unk 는 동일 id\n",
    "\n",
    "    def save(self, path):\n",
    "        super().save(path)\n",
    "        with open(os.path.join(path,\"word2idx.json\"), \"w\") as f:\n",
    "            json.dump(self.vocab, f)\n",
    "        with open(os.path.join(path,\"sample_weights.json\"), \"w\") as f:\n",
    "            json.dump(self.sample_weights, f)\n",
    "        \n",
    "    \n",
    "    def load(self, path):\n",
    "        super().load(path)\n",
    "        with open(os.path.join(path,\"word2idx.json\"), \"r\") as f:\n",
    "            self.vocab = json.load(f)\n",
    "        with open(os.path.join(path,\"sample_weights.json\"), \"r\") as f:\n",
    "            self.sample_weights = json.load(f)\n",
    "\n",
    "'''\n",
    "SentencePiece 의 주요 내장 메서드\n",
    "get_piece_size()\t전체 어휘 크기 반환\n",
    "encode(text)\t텍스트 → 토큰 ID 리스트\n",
    "decode(ids)\t토큰 ID 리스트 → 텍스트\n",
    "bos_id()\tBOS 토큰 ID\n",
    "eos_id()\tEOS 토큰 ID\n",
    "load(path)\t모델 파일 로드\n",
    "'''\n",
    "class BPETokenizer(Tokenizer):\n",
    "    def __init__(self, max_vocab_size=10000, special_tokens=[]):\n",
    "        super().__init__(max_vocab_size, special_tokens)\n",
    "        self.bpe = None #SentencePiece BPE 모델 저장할 변수\n",
    "        self.sample_weights = None #토큰별 가중치 저장 예정\n",
    "        assert len(special_tokens) == len(set(special_tokens)), \"Duplicate special tokens are not allowed.\" #중복 토큰 확인 없으면 에러 메시지\n",
    "        assert len(special_tokens) < max_vocab_size, \"Special tokens exceed max vocab size.\" #토큰 개수 제한 만약 넘어가면 에러 메세지\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.bpe.get_piece_size() #전체 어휘 크기 반환\n",
    "\n",
    "    def fit(self,dataset, save_path):\n",
    "        print(\"Training BPE Tokenizer...\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        prefix = os.path.join(save_path,\"bpe\")\n",
    "        sentencepiece.SentencePieceTrainer.train(\n",
    "            sentence_iterator=iter(dataset), #학습 데이터\n",
    "            model_prefix=prefix, #모델 저장 경로\n",
    "            vocab_size=self.max_vocab_size, #최대 어휘 크기\n",
    "            max_sentence_length=100000, #최대 문장 길이\n",
    "            shuffle_input_sentence=False, #입력 순서 유지\n",
    "            byte_fallback=True, #unknown 문자를 byte 로 처리, 즉 안녕하세요 라는 oov 도 [<0XA9>,<0XW8>...] 이런식으로 처리 가능하다는 말\n",
    "            num_threads=32,\n",
    "            pad_id=0,\n",
    "            pad_piece=\"<|PAD|>\",\n",
    "            unk_id=1,\n",
    "            unk_piece=\"<|UNK|>\",\n",
    "            bos_id=2,\n",
    "            bos_piece=\"<|BOS|>\",\n",
    "            eos_id=3,\n",
    "            eos_piece=\"<|EOS|>\",\n",
    "            user_defined_symbols=self.additional_special_tokens\n",
    "        )\n",
    "        self.bpe = sentencepiece.SentencePieceProcessor() #모델 객체 생성\n",
    "        self.bpe.load(prefix + \".model\")\n",
    "        with open(prefix+\".vocab\", \"r\", encoding=\"utf-8\") as f:\n",
    "            self.vocab = {}\n",
    "            self.sample_weights = []\n",
    "            for l in f:\n",
    "                token, weight = l.strip().split(\"\\t\")\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "                self.sample_weights.append(weight if token not in self.special_tokens else -987654321)\n",
    "\n",
    "    def tokenize(self, text, add_special_tokens=False):\n",
    "        tokens = self.bpe.encode(text, out_type=int)\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.bpe.bos_id] + tokens + [self.bpe.eos_id]\n",
    "        return tokens\n",
    "\n",
    "    def save(self, path):\n",
    "        super().save(path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        super().load(path)\n",
    "        self.bpe = sentencepiece.SentencePieceProcessor()\n",
    "        self.bpe.load(os.path.join(path,\"bpe.model\"))\n",
    "        with open(os.path.join(path,\"bpe.vocab\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            self.vocab = {}\n",
    "            self.sample_weights = []\n",
    "            for l in f:\n",
    "                token, weight = l.strip().split(\"\\t\")\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "                self.sample_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"abisee/cnn_dailymail\",'3.0.0')\n",
    "corpus_train_dataset = dataset['train'].select(range(50000))\n",
    "corpus_vaildation_dataset = dataset['validation']\n",
    "\n",
    "if os.path.exists(\"output/whitespace_tokenizer\"):\n",
    "    white_space_tokenizer = WitespaceTokenizer()\n",
    "    white_space_tokenizer.load(\"output/whitespace_tokenizer\")\n",
    "else:\n",
    "    white_space_tokenizer = WitespaceTokenizer(tokenizer_args['min_freq'],tokenizer_args['vocab_size'])\n",
    "    white_space_tokenizer.fit(corpus_train_dataset['article'])\n",
    "    white_space_tokenizer.save(\"output/whitespace_tokenizer\")\n",
    "\n",
    "if os.path.exists(\"output/bpe_tokenizer\"):\n",
    "    bpe_tokenizer = BPETokenizer()\n",
    "    bpe_tokenizer.load(\"output/bpe_tokenizer\")\n",
    "else:\n",
    "    bpe_tokenizer = BPETokenizer(tokenizer_args['vocab_size'])\n",
    "    bpe_tokenizer.fit(corpus_train_dataset['article'], \"output/bpe_tokenizer\")\n",
    "    bpe_tokenizer.save(\"output/bpe_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/9370 [15:10<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "중심 단어로 주변 단어 예측\n",
    "네거티브 샘플링 추가 즉 상관 없는 단어들 배치\n",
    "examples['article'] 이 해당 데이터 칼럼에 text 가 들어가 있음\n",
    "'''\n",
    "def sliding_window_preprocess(examples, tokenizer, window_size=2, num_negative_samples=-1):\n",
    "    contexts = [] #주변 단어들\n",
    "    targets = [] #중심 단어\n",
    "    negatives = [] #네거티브 샘플\n",
    "    for example in examples['article']:\n",
    "        tokens = tokenizer.tokenize(example)\n",
    "        for i in range(window_size, len(tokens) - window_size):\n",
    "            context = tokens[i-window_size:i] + tokens[i+1:i+window_size+1]\n",
    "            target = tokens[i]\n",
    "            contexts.append(context)\n",
    "            targets.append(target)\n",
    "            \n",
    "            if num_negative_samples > 0:\n",
    "                sampling_weight = copy.deepcopy(tokenizer.sample_weights)\n",
    "                for t in tokens[i-window_size:i+window_size+1]:\n",
    "                    sampling_weight[t] = -987654321\n",
    "                negatives.append(random.choices(range(len(tokenizer)), weights=sampling_weight, k=num_negative_samples))\n",
    "            else:\n",
    "                negatives.append([])\n",
    "    return {\n",
    "        \"context\": contexts,\n",
    "        \"target\": targets,\n",
    "        \"negative\": negatives,\n",
    "    }\n",
    "def sliding_window_collate_fn(batch):\n",
    "    contexts = torch.LongTensor([k['context'] for k in batch])\n",
    "    targets = torch.LongTensor([k['target'] for k in batch])\n",
    "    negatives = torch.LongTensor([k['negative'] for k in batch]) if batch[0]['negative'] else None\n",
    "    return {\n",
    "        \"context\": contexts,\n",
    "        \"target\": targets,\n",
    "        \"negative\": negatives,\n",
    "    }\n",
    "\n",
    "sw_wt_train_dataset = corpus_train_dataset.map(sliding_window_preprocess,\n",
    "                                 batched=True,\n",
    "                                 num_proc=os.cpu_count()//2,\n",
    "                                 remove_columns=corpus_train_dataset.column_names,\n",
    "                                 fn_kwargs={\"tokenizer\": white_space_tokenizer, \"window_size\": 2, \"num_negative_samples\": -1})\n",
    "\n",
    "sw_wt_valid_dataset = corpus_vaildation_dataset.map(sliding_window_preprocess,\n",
    "                                 batched=True,\n",
    "                                 num_proc=os.cpu_count()//2,\n",
    "                                 remove_columns=corpus_vaildation_dataset.column_names,\n",
    "                                 fn_kwargs={\"tokenizer\": white_space_tokenizer, \"window_size\": 2, \"num_negative_samples\": -1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for SkipGram with Negative Sampling (nn.Module) -- Your codes are required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, context, target, negative):\n",
    "        context_embeds = self.embeddings(context) #임베딩 들어가기\n",
    "        context_mean = context_embeds.mean(dim=1) #평균 벡터 계산\n",
    "        target_embed = self.embeddings(target) #중심 단어 임베딩\n",
    "        if negative is not None:\n",
    "            negative_embeds = self.embeddings(negative) #네거티브 샘플 임베딩\n",
    "            positive_score = torch.sum(context_mean * target_embed, dim=1) #정답 예측\n",
    "            positive_loss = -F.logsigmoid(positive_score)\n",
    "            negative_score = torch.bmm(negative_embeds, context_mean.unsqueeze(2)).squeeze(2) #네거티브 샘플 예측\n",
    "            negative_loss = -F.logsigmoid(-negative_score).sum(dim=1)\n",
    "            loss = (positive_loss + negative_loss).mean()\n",
    "        else:\n",
    "            # Negative Sampling 없이 Cross-Entropy Loss 사용\n",
    "            logits = torch.matmul(context_mean, self.embeddings.weight.T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "        ## YOUR CODES END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, valid_dataset, collate_fn, train_args, prefix):\n",
    "    optimzier = optim.Adam(model.parameters(), lr=train_args[\"lr\"])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=train_args['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=0)  # Windows 호환\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=train_args['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=0)  # Windows 호환\n",
    "\n",
    "    total_steps = len(train_dataloader) * train_args['epochs']\n",
    "\n",
    "    best_loss = 987654321\n",
    "    \n",
    "    output_path = os.path.join(\"output\", prefix)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    with open(os.path.join(output_path, \"train_args.json\"), \"w\") as f:\n",
    "        json.dump(train_args, f)\n",
    "\n",
    "    pbar = tqdm.tqdm(total=total_steps, desc=\"training\")\n",
    "    for epoch in range(train_args['epochs']):\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{train_args['epochs']}\")\n",
    "        move_avg_loss = []\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            batch = {k:v.to(train_args['device']) if isinstance(v,torch.Tensor) else v for k,v in batch.items()}\n",
    "\n",
    "            loss = model(**batch)\n",
    "            loss = loss / train_args['gradient_accumulate_steps']\n",
    "            if loss.size() != torch.Size([]):\n",
    "                loss = loss.mean()\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i+1) % train_args['gradient_accumulate_steps'] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "                optimzier.step()\n",
    "                optimzier.zero_grad()\n",
    "\n",
    "            move_avg_loss.append(loss.item()) \n",
    "            if len(move_avg_loss) > 100: move_avg_loss.pop(0)\n",
    "            pbar.set_postfix_str(f\"loss: {sum(move_avg_loss)/len(move_avg_loss):.04f} lr: {optimzier.param_groups[0]['lr']:.2e}\")\n",
    "            pbar.update(1)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            eval_loss = 0\n",
    "            for i, batch in enumerate(valid_dataloader):\n",
    "                with torch.no_grad():\n",
    "                    batch = {k:v.to(train_args['device']) if isinstance(v,torch.Tensor) else v for k,v in batch.items()}\n",
    "                    loss = model(**batch)\n",
    "                    if loss.size() != torch.Size([]):\n",
    "                        loss = loss.mean()\n",
    "                    eval_loss += loss.item()\n",
    "                    pbar.set_postfix_str(f\"val_loss: {eval_loss / (i+1):.04f}\")\n",
    "        eval_loss /= len(valid_dataloader)\n",
    "        pbar.write(f\"Validation Loss: {eval_loss:.04f}\")\n",
    "\n",
    "        if eval_loss < best_loss:\n",
    "            best_loss = eval_loss\n",
    "            \n",
    "            torch.save(model.state_dict(), os.path.join(output_path,\"best_model.pth\"))\n",
    "            pbar.write(f\"Model Saved best loss: {best_loss:.04f}\")\n",
    "\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 7611/1199360 [31:01<150:58:09,  2.19it/s, loss: 25.1760 lr: 1.00e-03]"
     ]
    }
   ],
   "source": [
    "sg_wt_model = SkipGram(vocab_size=len(white_space_tokenizer), embedding_dim=embedding_training_args['embedding_dim'])\n",
    "embedding_training_args['device'] = 'cpu'  # RTX 5070 CUDA 호환성 문제로 CPU 사용\n",
    "sg_wt_model = sg_wt_model.to(embedding_training_args['device'])\n",
    "sg_wt_model = nn.DataParallel(sg_wt_model) if torch.cuda.device_count() > 1 else sg_wt_model\n",
    "# sg_wt_model = torch.compile(sg_wt_model)  # Windows 호환성 문제로 비활성화\n",
    "train(sg_wt_model, sw_wt_train_dataset, sw_wt_valid_dataset, sliding_window_collate_fn, embedding_training_args, \"sg_wt\")\n",
    "best_model_statedict = torch.load(\"output/sg_wt/best_model.pth\")\n",
    "sg_wt_model.load_state_dict(best_model_statedict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
